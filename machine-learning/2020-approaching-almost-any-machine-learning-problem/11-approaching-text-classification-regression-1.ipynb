{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgzPez6jcGpz"
   },
   "source": [
    "# 11. Approaching Text Classification/Regression 1\n",
    "\n",
    "Text data can be categorized as tabular with having more structure. Contrary to previous examples so far, explanotary variables/inputs are not numerical but textual. Since ML algorithms need numbers to work on, we must devise a process to turn textual data into numerical values. Let's check the dataset used in this chapter. \n",
    "\n",
    "**Concepts:**\n",
    "\n",
    "- [Tokenization](#Tokenization)\n",
    "- [Bag of words](#Bag-of-words)\n",
    "- [Pre-tokenization]()\n",
    "- [Term frequency, inverse document frequency]()\n",
    "- [N-grams]()\n",
    "- [Stemming and lemmatization]()\n",
    "- [Topic extraction]()\n",
    "- [Latent Semantic Analysis]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T07:05:13.242596Z",
     "iopub.status.busy": "2021-05-24T07:05:13.242078Z",
     "iopub.status.idle": "2021-05-24T07:05:15.149727Z",
     "shell.execute_reply": "2021-05-24T07:05:15.148606Z",
     "shell.execute_reply.started": "2021-05-24T07:05:13.242513Z"
    },
    "executionInfo": {
     "elapsed": 3997,
     "status": "ok",
     "timestamp": 1621611483380,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "t-jtz_KdchD0",
    "outputId": "ba1003af-f6bd-4091-a362-03b577ffc804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import nltk # Natural Language Toolkit\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4e7eVdH8A7x"
   },
   "source": [
    "Main dataset is **Imdb Review Dataset**. There is a single feature column named as review and target column named as sentiment. Target distributon is 50/50 so it's a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:10:31.249249Z",
     "iopub.status.busy": "2021-05-23T16:10:31.248984Z",
     "iopub.status.idle": "2021-05-23T16:10:31.962892Z",
     "shell.execute_reply": "2021-05-23T16:10:31.961867Z",
     "shell.execute_reply.started": "2021-05-23T16:10:31.249223Z"
    },
    "id": "sLvKmmEm70rx"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/imdb.csv')\n",
    "df_raw.sentiment = df_raw.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\n",
    "df = df_raw.copy(deep=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5baex49Y-6r"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "As a start, we can decompose a text into sentences, then into words. This process is called tokenizing. After that we formulate a mapping between tokens and their numerical values. In order to tokenize a string of words, we use `nltk` library's `word_tokenize` method. As opposed to simple string `split` method, `word_tokenize` also considers punctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:10:31.965318Z",
     "iopub.status.busy": "2021-05-23T16:10:31.964912Z",
     "iopub.status.idle": "2021-05-23T16:10:31.98337Z",
     "shell.execute_reply": "2021-05-23T16:10:31.982391Z",
     "shell.execute_reply.started": "2021-05-23T16:10:31.965275Z"
    },
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1621597582839,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "LQIcK2XRewpp",
    "outputId": "c4356bde-6f47-4dd2-ab28-ef3aca356f71"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = 'hi, how are you?'\n",
    "\n",
    "print(sentence.split())\n",
    "\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KCwIKtBrVzE"
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "`CountVectorizer` processes a number of documents, in this case strings, and outputs a vector for each document and collects the results in a matrix. Every row vector is called **bag of words** and includes all the terms in all the documents. Matrices produced by `CountVectorizer` and the coming `TfidfVectorizer` are called **term-document** matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:10:31.985106Z",
     "iopub.status.busy": "2021-05-23T16:10:31.984829Z",
     "iopub.status.idle": "2021-05-23T16:10:32.000954Z",
     "shell.execute_reply": "2021-05-23T16:10:31.999666Z",
     "shell.execute_reply.started": "2021-05-23T16:10:31.985079Z"
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1621401453861,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "SR-LcrPOSrdF",
    "outputId": "20dd8f8f-59c0-45e2-fe7c-d1886791927c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "          'hello, how are you?',\n",
    "          'hello did you know about counts',\n",
    "          'YES!!!'\n",
    "]\n",
    "\n",
    "ctv = CountVectorizer()\n",
    "\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print('Sparse matrix')\n",
    "print(corpus_transformed)\n",
    "print('Vocabulary values')\n",
    "print(sorted(ctv.vocabulary_.items(), key=lambda x: x[1]))\n",
    "print('Array form')\n",
    "print(corpus_transformed.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYXnyFrBaJ6f"
   },
   "source": [
    "## Pre-tokenization\n",
    "\n",
    "Logistic regression code example in the book is not very efficient and takes too long due to tokenization of dataset over and over again. The catch is that tokenization is the same process for every iteration and can be performed in advance which is called **pre-tokenization**. Therefore, we tokenize the dataset beforehand and use kfold indexing during loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:10:32.002676Z",
     "iopub.status.busy": "2021-05-23T16:10:32.002384Z",
     "iopub.status.idle": "2021-05-23T16:13:06.210186Z",
     "shell.execute_reply": "2021-05-23T16:13:06.209229Z",
     "shell.execute_reply.started": "2021-05-23T16:10:32.002646Z"
    },
    "id": "6iBKEANKt23n"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "cvt= CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "df_tokenized = cvt.fit_transform(df.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eF7ACiSDIfBM"
   },
   "source": [
    "For `LogisticRegression`, default value of `max_iter` is 100 which is not sufficient for our example. If you run the code without changing iteration number, a warning will pop up regarding non convergance. For convergance, `max_iter = 10_000` should be chosen in case runtime is not an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:13:06.212339Z",
     "iopub.status.busy": "2021-05-23T16:13:06.211926Z",
     "iopub.status.idle": "2021-05-23T16:14:09.479034Z",
     "shell.execute_reply": "2021-05-23T16:14:09.477927Z",
     "shell.execute_reply.started": "2021-05-23T16:13:06.212297Z"
    },
    "executionInfo": {
     "elapsed": 63344,
     "status": "ok",
     "timestamp": 1621404286800,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "GRdGYbuNS-D1",
    "outputId": "ea3f5773-37cc-4f69-c9df-7ff1d7aa9364"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=df, y=y)):\n",
    "\n",
    "\n",
    "    X_train = df_tokenized[train_, :]\n",
    "    y_train = df.sentiment[train_]\n",
    " \n",
    "    X_test = df_tokenized[validate_, :]\n",
    "    y_test = df.sentiment[validate_]\n",
    "\n",
    "    model = linear_model.LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    print(f'Fold: {fold_}')\n",
    "    print(f'Accuracy {accuracy}')\n",
    "\n",
    "print(f'Mean accuracy {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE89xpWieeWR"
   },
   "source": [
    "Without pre tokenization `LogisticRegression` takes a long time, but `df_tokenized` decreases the runtime drastically. Now, as a second method we use `naive_bayes` that finishes instantaneously and considering the accuracy is very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:14:09.481033Z",
     "iopub.status.busy": "2021-05-23T16:14:09.480625Z",
     "iopub.status.idle": "2021-05-23T16:14:10.024531Z",
     "shell.execute_reply": "2021-05-23T16:14:10.023532Z",
     "shell.execute_reply.started": "2021-05-23T16:14:09.480987Z"
    },
    "executionInfo": {
     "elapsed": 156226,
     "status": "ok",
     "timestamp": 1621335070208,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "px3sVv-XioA8",
    "outputId": "763bb03b-0e60-47e5-e7b4-4c6633a53648"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=df, y=y)):\n",
    "\n",
    "\n",
    "    X_train = df_tokenized[train_, :]\n",
    "    y_train = df.sentiment[train_]\n",
    " \n",
    "    X_test = df_tokenized[validate_, :]\n",
    "    y_test = df.sentiment[validate_]\n",
    "\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    print(f'Fold: {fold_}')\n",
    "    print(f'Accuracy {accuracy}')\n",
    "\n",
    "print(f'Mean accuracy {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK6rBvIbBE0Y"
   },
   "source": [
    "## Term Frequency and Inverse Document Frequency\n",
    "\n",
    "In additon to word count used in bag of words approach, we can use frequency values of words both regarding the document itself and the collection of documents. Following metrics are useful in this approach:\n",
    "\n",
    "- Term frequency (**TF**) concerns with a single document and is the number of times a term occurs in a document: \n",
    "$$TF(t) = \\frac{\\text{# of times t occur}}{\\text{# of terms}} \\tag{11-1}$$ \n",
    "\n",
    "- Inverse document frequencey (**IDF**) concerns with a collection of documents and is logarithm of the number of documents divided by the number of documents including the given term t:\n",
    "\n",
    "$$IDF(t) = \\log \\left(\\frac{\\text{# of documents}}{\\text{# of documents with t in it}} \\right) \\tag{11-2}$$\n",
    "\n",
    "- Finally, **TF-IDF** combines the two metrics into one.\n",
    "\n",
    "$$TF\\text{-}IDF(t) = TF(t) \\times IDF(t) \\tag{11-3}$$\n",
    "\n",
    "Subtlety of **TF-IDF** lies in its capacity to consider and combine informaton about the whole collection of documents. In bag of words approach, we are only able to assign a value to a token regarding single document. In **TF-IDF**, numerical value is assigned according to its occurence in all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:14:10.02702Z",
     "iopub.status.busy": "2021-05-23T16:14:10.026691Z",
     "iopub.status.idle": "2021-05-23T16:14:10.040777Z",
     "shell.execute_reply": "2021-05-23T16:14:10.039618Z",
     "shell.execute_reply.started": "2021-05-23T16:14:10.026991Z"
    },
    "executionInfo": {
     "elapsed": 156224,
     "status": "ok",
     "timestamp": 1621335070211,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "jn3czunDFGrg",
    "outputId": "1efdc8e0-7a53-48b0-f54e-b55d4d1fe64a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [\n",
    "          \"hello, how are you?\",\n",
    "          \"im getting bored at home. And you? What do you think?\",\n",
    "          \"did you know about counts\",\n",
    "          \"let's see if this works!\",\n",
    "          \"YES!!!!\"\n",
    "]\n",
    "\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "tfv.fit(corpus)\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1uECZniTtul"
   },
   "source": [
    "We use the same pre tokenization approach again here with `TfidVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:14:10.042812Z",
     "iopub.status.busy": "2021-05-23T16:14:10.042526Z",
     "iopub.status.idle": "2021-05-23T16:16:43.681303Z",
     "shell.execute_reply": "2021-05-23T16:16:43.680305Z",
     "shell.execute_reply.started": "2021-05-23T16:14:10.042783Z"
    },
    "id": "9PKHuUK3ToNw"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tvt = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "df_tfidf = tvt.fit_transform(df.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fKuI7T2OtyV"
   },
   "source": [
    "Now, we can feed tokenized dataset to `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:16:43.683024Z",
     "iopub.status.busy": "2021-05-23T16:16:43.682732Z",
     "iopub.status.idle": "2021-05-23T16:16:54.421282Z",
     "shell.execute_reply": "2021-05-23T16:16:54.420374Z",
     "shell.execute_reply.started": "2021-05-23T16:16:43.682995Z"
    },
    "executionInfo": {
     "elapsed": 257907,
     "status": "ok",
     "timestamp": 1621335171905,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "U8yrkR3KG2Fw",
    "outputId": "fa83c68d-d988-4fd0-855e-527d5c44361f"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=df, y=y)):\n",
    "\n",
    "\n",
    "    X_train = df_tfidf[train_, :]\n",
    "    y_train = df.sentiment[train_]\n",
    " \n",
    "    X_test = df_tfidf[validate_, :]\n",
    "    y_test = df.sentiment[validate_]\n",
    "\n",
    "    model = linear_model.LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    print(f'Fold: {fold_}')\n",
    "    print(f'Accuracy {accuracy}')\n",
    "\n",
    "print(f'Mean accuracy {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2JvtlikaaFt"
   },
   "source": [
    "## N-grams\n",
    "\n",
    "Next tokenization approach is **n-grams**. Instead fo simple word counts, **n-grams** also take into account order by including n-1 surronding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:16:54.42277Z",
     "iopub.status.busy": "2021-05-23T16:16:54.422487Z",
     "iopub.status.idle": "2021-05-23T16:16:54.428788Z",
     "shell.execute_reply": "2021-05-23T16:16:54.427739Z",
     "shell.execute_reply.started": "2021-05-23T16:16:54.422742Z"
    },
    "executionInfo": {
     "elapsed": 257905,
     "status": "ok",
     "timestamp": 1621335171908,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "QQAvKqvOmO14",
    "outputId": "02ab384e-bfa1-430c-d887-70616bea1723"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "N = 3\n",
    "\n",
    "sentence = 'Hi, how are you?'\n",
    "\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BXUEFjIyMxA"
   },
   "source": [
    "Once again we pre tokenize the dataset. This time we add n-gram argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:16:54.43044Z",
     "iopub.status.busy": "2021-05-23T16:16:54.430122Z",
     "iopub.status.idle": "2021-05-23T16:20:06.205281Z",
     "shell.execute_reply": "2021-05-23T16:20:06.204253Z",
     "shell.execute_reply.started": "2021-05-23T16:16:54.43041Z"
    },
    "id": "4s_oHi3RvDyn"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tvt_ngram = TfidfVectorizer(tokenizer=word_tokenize, \n",
    "                            token_pattern=None, \n",
    "                            ngram_range=(1,2))\n",
    "\n",
    "df_tfidf_ngrams = tvt_ngram.fit_transform(df.review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9lzzkVmyYaW"
   },
   "source": [
    "One should note the size difference between `df_tfidf` and `df_tfidf_ngrams`. The first one is `(50000, 168707)` and the second one is `(50000, 2348110)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:20:06.206982Z",
     "iopub.status.busy": "2021-05-23T16:20:06.206674Z",
     "iopub.status.idle": "2021-05-23T16:20:51.950311Z",
     "shell.execute_reply": "2021-05-23T16:20:51.949357Z",
     "shell.execute_reply.started": "2021-05-23T16:20:06.206951Z"
    },
    "executionInfo": {
     "elapsed": 459141,
     "status": "ok",
     "timestamp": 1621335373152,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "6L8yB8Rzvo4D",
    "outputId": "43dd37c4-df0e-4a8d-ab4d-06621aae88e7"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=df, y=y)):\n",
    "\n",
    "\n",
    "    X_train = df_tfidf_ngrams[train_, :]\n",
    "    y_train = df.sentiment[train_]\n",
    " \n",
    "    X_test = df_tfidf_ngrams[validate_, :]\n",
    "    y_test = df.sentiment[validate_]\n",
    "\n",
    "    model = linear_model.LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    print(f'Fold: {fold_}')\n",
    "    print(f'Accuracy {accuracy}')\n",
    "\n",
    "print(f'Mean accuracy {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:20:51.95204Z",
     "iopub.status.busy": "2021-05-23T16:20:51.951747Z",
     "iopub.status.idle": "2021-05-23T16:20:54.066362Z",
     "shell.execute_reply": "2021-05-23T16:20:54.065444Z",
     "shell.execute_reply.started": "2021-05-23T16:20:51.952003Z"
    },
    "executionInfo": {
     "elapsed": 461187,
     "status": "ok",
     "timestamp": 1621335375202,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "M-txZP9p7lNA",
    "outputId": "5414d8ef-004d-46a3-c441-efa6e832b849"
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "words = ['fishing', 'fishes', 'fished']\n",
    "\n",
    "for word in words:\n",
    "    print(f'word={word}')\n",
    "    print(f'stemmed word={stemmer.stem(word)}')\n",
    "    print(f'lemma={lemmatizer.lemmatize(word)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OsZz9l1GeVW"
   },
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "This section is a little too terse if you are not familiar with the concepts and also lacks motivation. Before proceeding to dive into the code, some explanation about SVD and TruncatedSVD is necessary. Singular Value Decompositon (SVD) is a matrix decompostion method such that given a dataset matrix $X$ produces a low rank approximation to $X$. \n",
    "\n",
    "Assuming $m$ is number of documents and $n$ is the number of unique terms, our data matrix (term-document matrix produced by a `Vectorizer`) $X$ can be decomposed with a given `n_components=t` as follows:\n",
    "\n",
    "$$\\underbrace{\\underbrace{X}_{(m \\times n)} \\approx X_{t} = \\underbrace{U_{t}\\Sigma_{t}V_{t}^{\\intercal}}_{(m \\times t) ~ (t\\times t) ~ (t\\times n)}}_{\\text{Dimensions}}   \\tag{11-4}$$\n",
    "\n",
    "When SVD applied to term-document matrices as we obtain by running `CountVectorizer` or `TfidfVectorizer`, it's called latent semantic analysis. Purpose of this approach is to reduce the dimensionality of data matrix. As we have seen the matrices produced by `Vectorizers` have large number of columns and took too long to process. Another advantage is that `SVD` can work with sparse matrices and therefore can deal with sizable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:20:54.067908Z",
     "iopub.status.busy": "2021-05-23T16:20:54.067614Z",
     "iopub.status.idle": "2021-05-23T16:21:57.449211Z",
     "shell.execute_reply": "2021-05-23T16:21:57.448126Z",
     "shell.execute_reply.started": "2021-05-23T16:20:54.067879Z"
    },
    "executionInfo": {
     "elapsed": 40870,
     "status": "ok",
     "timestamp": 1621340265449,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "Z7taX7tVGg1v",
    "outputId": "9f9f9588-74a3-4d50-b8ff-bb19299ae5de"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = df.review.values[:10_000]\n",
    "\n",
    "tfv =TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "for sample_index in range(5):\n",
    "    feature_scores = dict(zip(tfv.get_feature_names(), \n",
    "                          corpus_svd.components_[sample_index]))\n",
    "\n",
    "    print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpUF2ldFapsI"
   },
   "source": [
    "We can clear punctuation and other non letter characters using `apply` method of pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:21:57.450868Z",
     "iopub.status.busy": "2021-05-23T16:21:57.450576Z",
     "iopub.status.idle": "2021-05-23T16:29:12.130184Z",
     "shell.execute_reply": "2021-05-23T16:29:12.129141Z",
     "shell.execute_reply.started": "2021-05-23T16:21:57.45084Z"
    },
    "id": "YjNRtcvDhNcp"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.split()\n",
    "    s = ' '.join(s)\n",
    "    s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "    return s\n",
    "\n",
    "corpus = df.copy(deep=True)\n",
    "\n",
    "corpus.loc[:, 'review'] = corpus.review.apply(clean_text)\n",
    "\n",
    "corpus = corpus.review.values\n",
    "\n",
    "tfv =TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=1000)\n",
    "\n",
    "df_reduced = svd.fit_transform(corpus_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-LkEhBq7qcR"
   },
   "source": [
    "After tranfromation, we run `LogisticRegression` with reduced dataset and set `n_components=1000` to retain 0.89 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-23T16:29:12.131566Z",
     "iopub.status.busy": "2021-05-23T16:29:12.131292Z",
     "iopub.status.idle": "2021-05-23T16:29:57.236792Z",
     "shell.execute_reply": "2021-05-23T16:29:57.230711Z",
     "shell.execute_reply.started": "2021-05-23T16:29:12.13154Z"
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "error",
     "timestamp": 1621404302311,
     "user": {
      "displayName": "Anton Lavrentyevich",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjykfk2GGM9LVcxnHX4ZWaFA2zpO8vcawyCuYysWA=s64",
      "userId": "01617178466349505698"
     },
     "user_tz": -180
    },
    "id": "xlW1TU13jqV2",
    "outputId": "821cd8d8-4f7a-439e-a64b-abb175ccf481"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=df, y=y)):\n",
    "\n",
    "\n",
    "    X_train = df_reduced[train_, :]\n",
    "    y_train = df.sentiment[train_]\n",
    " \n",
    "    X_test = df_reduced[validate_, :]\n",
    "    y_test = df.sentiment[validate_]\n",
    "\n",
    "    model = linear_model.LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    print(f'Fold: {fold_}')\n",
    "    print(f'Accuracy {accuracy}')\n",
    "\n",
    "print(f'Mean accuracy {sum(accuracies)/len(accuracies)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
