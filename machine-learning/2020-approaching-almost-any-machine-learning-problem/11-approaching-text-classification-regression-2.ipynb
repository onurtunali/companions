{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "native-context",
   "metadata": {
    "papermill": {
     "duration": 0.014465,
     "end_time": "2021-05-24T19:56:09.757419",
     "exception": false,
     "start_time": "2021-05-24T19:56:09.742954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 11. Approaching Text Classification/Regression 2\n",
    "\n",
    "In NLP, every document, text or a string (text object in short) signify an instance which is a row in our dataset representation as a matrix. \n",
    "\n",
    "As previously explained, bag of words approach tokenizes the text object and uses every token as a feature. Token counts of the text object are accepted as feature value. Naturally, size is dependent on the number of tokens unsless limited beforehand. \n",
    "\n",
    "Representing a word with a vector is called **word embedding**. At the end, we have a dictionary with keys corresponding to words and values corresponding to vectors. The representation of whole document is found by adding all word vectors present in the document. Important part is to find the embedding vectors by reconstructing input sentences.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "- [Word Embeddings]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "copyrighted-conference",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T19:56:09.787659Z",
     "iopub.status.busy": "2021-05-24T19:56:09.786499Z",
     "iopub.status.idle": "2021-05-24T19:56:09.799733Z",
     "shell.execute_reply": "2021-05-24T19:56:09.800244Z",
     "shell.execute_reply.started": "2021-05-24T19:35:51.113248Z"
    },
    "papermill": {
     "duration": 0.030019,
     "end_time": "2021-05-24T19:56:09.800532",
     "exception": false,
     "start_time": "2021-05-24T19:56:09.770513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "better-policy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T19:56:09.831995Z",
     "iopub.status.busy": "2021-05-24T19:56:09.831331Z",
     "iopub.status.idle": "2021-05-24T19:56:12.224493Z",
     "shell.execute_reply": "2021-05-24T19:56:12.223929Z",
     "shell.execute_reply.started": "2021-05-24T19:35:58.620481Z"
    },
    "papermill": {
     "duration": 2.410556,
     "end_time": "2021-05-24T19:56:12.224660",
     "exception": false,
     "start_time": "2021-05-24T19:56:09.814104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset with folds determined\n",
    "df = pd.read_csv(os.path.join('data/imdb_folds.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "registered-clarity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T19:56:12.260110Z",
     "iopub.status.busy": "2021-05-24T19:56:12.259340Z",
     "iopub.status.idle": "2021-05-24T19:56:13.728296Z",
     "shell.execute_reply": "2021-05-24T19:56:13.727792Z",
     "shell.execute_reply.started": "2021-05-24T18:29:11.250025Z"
    },
    "papermill": {
     "duration": 1.490643,
     "end_time": "2021-05-24T19:56:13.728444",
     "exception": false,
     "start_time": "2021-05-24T19:56:12.237801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6882472 , 0.        , 0.22941573, 0.6882472 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    \n",
    "    words = str(s).lower()\n",
    "    words = tokenizer(words)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    M = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "            \n",
    "    if len(M) == 0:\n",
    "        M = np.zeroes(300)\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v**2).sum())\n",
    "\n",
    "\n",
    "# Let's concoct an example\n",
    "\n",
    "embedding_dict = {'new': [1,0,0,3],\n",
    "                 'example': [2,0,1,0]}\n",
    "\n",
    "s = ' This is the new example'\n",
    "\n",
    "sentence_to_vec(s, embedding_dict, [], word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-example",
   "metadata": {
    "papermill": {
     "duration": 0.013088,
     "end_time": "2021-05-24T19:56:13.755404",
     "exception": false,
     "start_time": "2021-05-24T19:56:13.742316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In order for this code to work, we need `wiki-news-300d-1M.vec` embeddings which is added to input folder. Additionaly, we cannot work with the entire embeddings due to RAM limitations. That's why we limit the number embedding vectors as opposed to the code given in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brazilian-cholesterol",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T19:56:13.795011Z",
     "iopub.status.busy": "2021-05-24T19:56:13.794211Z",
     "iopub.status.idle": "2021-05-24T20:04:34.857695Z",
     "shell.execute_reply": "2021-05-24T20:04:34.856648Z",
     "shell.execute_reply.started": "2021-05-24T19:06:59.671330Z"
    },
    "papermill": {
     "duration": 501.089127,
     "end_time": "2021-05-24T20:04:34.857962",
     "exception": false,
     "start_time": "2021-05-24T19:56:13.768835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n",
      "Creating sentence vectors\n",
      "Training fold 0\n",
      "Accuracy 0.8172\n",
      "\n",
      "Training fold 1\n",
      "Accuracy 0.8159\n",
      "\n",
      "Training fold 2\n",
      "Accuracy 0.8133\n",
      "\n",
      "Training fold 3\n",
      "Accuracy 0.8231\n",
      "\n",
      "Training fold 4\n",
      "Accuracy 0.8209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fasttext.py\n",
    "\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "limit = 10_000 # Number of words in Embedding dictionary\n",
    "\n",
    "def load_vectors(fname, limit):\n",
    "    count = 0\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "\n",
    "    for line in fin:\n",
    "        count += 1\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "        if count == limit:\n",
    "            break\n",
    "            \n",
    "    return data\n",
    "\n",
    "print('Loading embeddings')\n",
    "\n",
    "embedding_dict = load_vectors('data/wiki-news-300d-1M.vec', limit)\n",
    "\n",
    "print('Creating sentence vectors')\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for review in df.review.values:\n",
    "    vectors.append(\n",
    "        sentence_to_vec(s=review, \n",
    "                        embedding_dict=embedding_dict, \n",
    "                        stop_words=[], \n",
    "                        tokenizer=word_tokenize\n",
    "                        )\n",
    "        )\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "y = df.sentiment.values\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "for fold_, (train_, validate_) in enumerate(kf.split(X=vectors, y=y)):\n",
    "\n",
    "    print(f'Training fold {fold_}')\n",
    "    X_train = vectors[train_]\n",
    "    y_train = y[train_]\n",
    "\n",
    "    X_test = vectors[validate_]\n",
    "    y_test = y[validate_]\n",
    "    model = linear_model.LogisticRegression(solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, preds)\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-labor",
   "metadata": {
    "id": "JVHZRUodw5Vi",
    "papermill": {
     "duration": 0.025811,
     "end_time": "2021-05-24T20:04:34.909641",
     "exception": false,
     "start_time": "2021-05-24T20:04:34.883830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are working in notebook so `config` file will be coded as an object such that attributes can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "descending-criterion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:04:34.969897Z",
     "iopub.status.busy": "2021-05-24T20:04:34.968984Z",
     "iopub.status.idle": "2021-05-24T20:04:34.971149Z",
     "shell.execute_reply": "2021-05-24T20:04:34.971656Z",
     "shell.execute_reply.started": "2021-05-24T19:36:07.176021Z"
    },
    "id": "gxdKivUBxR13",
    "papermill": {
     "duration": 0.036409,
     "end_time": "2021-05-24T20:04:34.971808",
     "exception": false,
     "start_time": "2021-05-24T20:04:34.935399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, MAX_LEN, \n",
    "                 TRAIN_BATCH_SIZE,\n",
    "                 VALID_BATCH_SIZE,\n",
    "                 EPOCHS):\n",
    "        \n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "        self.VALID_BATCH_SIZE = VALID_BATCH_SIZE\n",
    "        self.EPOCHS = EPOCHS\n",
    "\n",
    "config = Config(MAX_LEN = 128, \n",
    "                TRAIN_BATCH_SIZE = 16, \n",
    "                VALID_BATCH_SIZE = 8, \n",
    "                EPOCHS = 1 )\n",
    "\n",
    "EMBEDDING_WORD_LIMIT = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-career",
   "metadata": {
    "id": "IYEMrWDzzb7x",
    "papermill": {
     "duration": 0.01445,
     "end_time": "2021-05-24T20:04:35.000683",
     "exception": false,
     "start_time": "2021-05-24T20:04:34.986233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`dataset` file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "checked-width",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:04:35.033729Z",
     "iopub.status.busy": "2021-05-24T20:04:35.032863Z",
     "iopub.status.idle": "2021-05-24T20:04:35.958831Z",
     "shell.execute_reply": "2021-05-24T20:04:35.958161Z",
     "shell.execute_reply.started": "2021-05-24T19:36:09.189479Z"
    },
    "id": "Ab1rbtYEzcZx",
    "papermill": {
     "duration": 0.943554,
     "end_time": "2021-05-24T20:04:35.958980",
     "exception": false,
     "start_time": "2021-05-24T20:04:35.015426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "class IMDBDataset:\n",
    "\n",
    "    def __init__(self, reviews, targets):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = self.reviews[item,:]\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        return {'review': torch.tensor(review, dtype=torch.long),\n",
    "                'target': torch.tensor(target, dtype=torch.float)}\n",
    "    \n",
    "EMBEDDING_WORD_LIMIT = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-gender",
   "metadata": {
    "id": "renlD2rpzvf7",
    "papermill": {
     "duration": 0.015111,
     "end_time": "2021-05-24T20:04:35.988968",
     "exception": false,
     "start_time": "2021-05-24T20:04:35.973857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`lstm` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interesting-break",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:04:36.028123Z",
     "iopub.status.busy": "2021-05-24T20:04:36.027335Z",
     "iopub.status.idle": "2021-05-24T20:04:36.030123Z",
     "shell.execute_reply": "2021-05-24T20:04:36.029456Z",
     "shell.execute_reply.started": "2021-05-24T19:36:16.922482Z"
    },
    "id": "k9bR47S4z2LJ",
    "papermill": {
     "duration": 0.026869,
     "end_time": "2021-05-24T20:04:36.030270",
     "exception": false,
     "start_time": "2021-05-24T20:04:36.003401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lstm.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, embedding_matrix):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        embed_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_words, \n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            128,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(512, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        avg_pool = torch.mean(x, 1)\n",
    "        max_pool, _ = torch.max(x, 1)\n",
    "\n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-installation",
   "metadata": {
    "id": "5QyxPUhp0aEH",
    "papermill": {
     "duration": 0.015294,
     "end_time": "2021-05-24T20:04:36.060829",
     "exception": false,
     "start_time": "2021-05-24T20:04:36.045535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`enigne.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "needed-investigation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:04:36.102787Z",
     "iopub.status.busy": "2021-05-24T20:04:36.102138Z",
     "iopub.status.idle": "2021-05-24T20:04:36.104170Z",
     "shell.execute_reply": "2021-05-24T20:04:36.104625Z",
     "shell.execute_reply.started": "2021-05-24T19:36:20.582613Z"
    },
    "id": "UeBuQGqVSZJx",
    "papermill": {
     "duration": 0.028407,
     "end_time": "2021-05-24T20:04:36.104779",
     "exception": false,
     "start_time": "2021-05-24T20:04:36.076372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# engine.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(data_loader, model, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in data_loader:\n",
    "\n",
    "        reviews = data['review']\n",
    "        targets = data['target']\n",
    "\n",
    "        reviews = reviews.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(reviews)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(\n",
    "            predictions,\n",
    "            targets.view(-1, 1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in data_loader:\n",
    "            \n",
    "            reviews = data['review']\n",
    "            targets = data['target']\n",
    "            \n",
    "            reviews = reviews.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            predictions = model(reviews)\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "\n",
    "    return final_predictions, final_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-metallic",
   "metadata": {
    "id": "8YmXN28Q25JE",
    "papermill": {
     "duration": 0.01466,
     "end_time": "2021-05-24T20:04:36.134420",
     "exception": false,
     "start_time": "2021-05-24T20:04:36.119760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`train.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "light-terrace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:04:36.167242Z",
     "iopub.status.busy": "2021-05-24T20:04:36.166590Z",
     "iopub.status.idle": "2021-05-24T20:05:03.191459Z",
     "shell.execute_reply": "2021-05-24T20:05:03.190978Z",
     "shell.execute_reply.started": "2021-05-24T19:36:23.373547Z"
    },
    "id": "hrIdg4Td21Hw",
    "outputId": "205d0a21-dcb8-4375-c91a-662d781fdd15",
    "papermill": {
     "duration": 27.041883,
     "end_time": "2021-05-24T20:05:03.191623",
     "exception": false,
     "start_time": "2021-05-24T20:04:36.149740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer\n",
      "Loading Embeddings\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-603eb80293fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-603eb80293fb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(df, fold)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def load_vectors(fname, limit):\n",
    "    count = 0\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', \n",
    "                  newline='\\n', errors='ignore')\n",
    "\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "\n",
    "    for line in fin:\n",
    "\n",
    "        count += 1\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "        if count > limit:\n",
    "            break\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_embedding_matrix(word_index, embedding_dict):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def run(df, fold):\n",
    "\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    print('Fitting Tokenizer')\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(df.review.values.tolist())\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(train_df.review.values)\n",
    "    X_test = tokenizer.texts_to_sequences(valid_df.review.values)\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_train, maxlen=config.MAX_LEN\n",
    "    )\n",
    "\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_test, maxlen=config.MAX_LEN\n",
    "    )\n",
    "\n",
    "\n",
    "    train_dataset = IMDBDataset(reviews=X_train,\n",
    "                                        targets=train_df.sentiment.values)\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=2)\n",
    "\n",
    "    valid_dataset = IMDBDataset(reviews=X_test,\n",
    "                                        targets=valid_df.sentiment.values)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=1)\n",
    "\n",
    "    print('Loading Embeddings'),\n",
    "    \n",
    "    embedding_dict= load_vectors('data/wiki-news-300d-1M.vec', EMBEDDING_WORD_LIMIT)\n",
    "    \n",
    "    embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    model = LSTM(embedding_matrix)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    print('Training model')\n",
    "    best_accuracy = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(config.EPOCHS):\n",
    "\n",
    "        train(train_data_loader, model, optimizer, device)\n",
    "        outputs, targets = evaluate(valid_data_loader, model, device)\n",
    "\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f'Fold: {fold}, Epoch:{epoch}, Accuracy: {accuracy}')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > 2:\n",
    "            break\n",
    "\n",
    "for fold in range(5):\n",
    "    run(df, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electrical-cigarette",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T20:05:03.233793Z",
     "iopub.status.busy": "2021-05-24T20:05:03.233198Z",
     "iopub.status.idle": "2021-05-24T20:05:03.236135Z",
     "shell.execute_reply": "2021-05-24T20:05:03.235624Z"
    },
    "id": "TOkDeRIvN1a2",
    "papermill": {
     "duration": 0.028156,
     "end_time": "2021-05-24T20:05:03.236249",
     "exception": false,
     "start_time": "2021-05-24T20:05:03.208093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_embeddings(word_index, embedding_file, vector_length=300):\n",
    "\n",
    "    max_features = len(word_index) + 1\n",
    "    words_to_find = list(word_index.keys())\n",
    "    more_words_to_find = []\n",
    "\n",
    "    for wtf in words_to_find:\n",
    "        \n",
    "        more_words_to_find.append(wtf)\n",
    "        more_words_to_find.append(str(wtf).capitalize())\n",
    "\n",
    "    more_words_to_find = set(more_words_to_find)\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.assarray(arr, dtype='float32')\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        get_coefs(*o.strip().split(' '))\n",
    "        for o in open(embedding_file)\n",
    "        if o.split(' ')[0]\n",
    "        in more_words_to_find\n",
    "        and len(0) > 100\n",
    "    )\n",
    "\n",
    "    embedding_matrix = np.zeros((max_features, vector_length))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(str(word).capitalize())\n",
    "\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = embeddings_index.get(str(word).upper())\n",
    "\n",
    "        if (embedding_vector is not None and len(embedding_vector) == vector_length):\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 542.984508,
   "end_time": "2021-05-24T20:05:04.998245",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-24T19:56:02.013737",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
