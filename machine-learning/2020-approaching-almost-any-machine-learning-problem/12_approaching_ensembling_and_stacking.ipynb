{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 12. Approaching Ensembling and Stacking\n\n**Ensembling** in short is combination of different models. We end up with a result by averaging the outputs of every model in simplest usage. Models shouldn't be correlated for better results. Two main ways for averagins:\n\n- If results are probabilities, then a simple mean might be used. \n- If results are predictions, then a majority voting might be used.\n\n","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:23:40.119576Z","iopub.execute_input":"2021-05-29T10:23:40.120059Z","iopub.status.idle":"2021-05-29T10:23:40.837400Z","shell.execute_reply.started":"2021-05-29T10:23:40.119975Z","shell.execute_reply":"2021-05-29T10:23:40.836599Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Every column indicates the prediction of a model for that instance (datapoint)\n\n|Instance | Model 1 | Model 2 | Model 3|\n|---|---|---|---|\n|$x_{1}$| 0.1| 0.3|0.1|\n|$x_{2}$| 0.9| 0.7|0.6|\n|$x_{2}$| 0.3| 0.3|0.4|\n\n\n```python\n# python code for the results\nresults = np.array([[0.1, 0.3, 0.1],\n                    [0.9, 0.7, 0.6],\n                    [0.3, 0.3, 0.4]])\n```","metadata":{}},{"cell_type":"code","source":"\nresult_lab = np.array([[0,0,1],\n                    [0,1,2],\n                    [2,2,2]])\n\nresult_prob = np.array([[0.1, 0.3, 0.1],\n                    [0.9, 0.7, 0.6],\n                    [0.3, 0.3, 0.4]])\n\n\ndef mean_pred(probas):\n    return np.mean(probas, axis=1)\n\n# The implementation in the book outputs maximum. This is majority voting\ndef max_voting(preds):\n    result = [0] * preds.shape[0]\n    for idx, item in enumerate(preds):\n        c = Counter(item)\n        result[idx] = c.most_common(1)[0][0]\n    return result\n\n\ndef rank_mean(probas):\n    \n    ranked = []\n    for i in range(probas.shape[1]):\n        rank_data = stats.rankdata(probas[:,i])\n        ranked.append(rank_data)\n    ranked = np.column_stack(ranked)\n    return np.mean(ranked, axis=1)\n\n\nprint(mean_pred(result_prob))\nprint(max_voting(result_lab))\nprint(rank_mean(result_prob))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T11:05:44.382736Z","iopub.execute_input":"2021-05-29T11:05:44.383117Z","iopub.status.idle":"2021-05-29T11:05:44.396460Z","shell.execute_reply.started":"2021-05-29T11:05:44.383087Z","shell.execute_reply":"2021-05-29T11:05:44.395483Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[0.16666667 0.73333333 0.33333333]\n[0, 0, 2]\n[1.16666667 3.         1.83333333]\n","output_type":"stream"}]},{"cell_type":"code","source":"from functools import partial\nfrom scipy.optimize import fmin\nfrom sklearn import metrics\n\nclass OptimizeAUC:\n    \n    def __init__(self):\n        self.coef_ = 0\n        \n        \n    def _auc(self, coef, X, y):\n        \n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        if len(set(y)) > 2:\n            auc_score = metrics.roc_auc_score(y, predictions, multi_class='ovr')\n        else:\n            auc_score = metrics.roc_auc_score(y, predictions)\n        \n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._auc, X=X, y=y)\n        initial_coef =  np.random.dirichlet(np.ones(X.shape[1]), size=1)\n        self.coef_ = fmin(loss_partial, initial_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:45:47.677685Z","iopub.execute_input":"2021-05-29T10:45:47.678020Z","iopub.status.idle":"2021-05-29T10:45:47.885754Z","shell.execute_reply.started":"2021-05-29T10:45:47.677991Z","shell.execute_reply":"2021-05-29T10:45:47.884781Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\n\n# Creating the toy dataset\nX, y = make_classification(n_samples=10_000, n_features=30)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.1, \n                                                    shuffle=True, \n                                                    stratify=y)\n\nclf = LogisticRegression(solver='sag')\n\nresults = cross_validate(clf, X_train, y_train, \n                         cv=5, \n                         scoring='accuracy', \n                         n_jobs=-1, \n                         return_estimator=True)\n\nfor idx, item in results.items():\n    print(f'{idx}: {item}')\n    \ntest_score = np.array(results['test_score'])\nmodel_idx = np.argmax(test_score)\nmodel = results['estimator'][model_idx]\nprint('\\n', model.get_params())\npreds = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, preds)\nprint(f'Test accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:48:48.788697Z","iopub.execute_input":"2021-05-29T10:48:48.789112Z","iopub.status.idle":"2021-05-29T10:48:50.968181Z","shell.execute_reply.started":"2021-05-29T10:48:48.789076Z","shell.execute_reply":"2021-05-29T10:48:50.967023Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"fit_time: [0.12819815 0.13134813 0.12959719 0.11404514 0.09048676]\nscore_time: [0.00116539 0.0011251  0.00106859 0.00079179 0.00072479]\nestimator: [LogisticRegression(solver='sag'), LogisticRegression(solver='sag'), LogisticRegression(solver='sag'), LogisticRegression(solver='sag'), LogisticRegression(solver='sag')]\ntest_score: [0.90055556 0.89166667 0.90666667 0.89555556 0.90277778]\n\n {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'sag', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\nTest accuracy: 0.897\n","output_type":"stream"}]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nX, y  = make_classification(n_samples=10_000, n_features=25)\n\nX_fold1, X_fold2, y_fold1, y_fold2 = train_test_split(X, y, test_size=0.5, stratify=y)\n\nlogreg = linear_model.LogisticRegression()\nrf = ensemble.RandomForestClassifier()\nxgbc = xgb.XGBClassifier()\n\nlogreg.fit(X_fold1, y_fold1)\nrf.fit(X_fold1, y_fold1)\nxgbc.fit(X_fold1, y_fold1)\n\npred_logreg = logreg.predict_proba(X_fold2)[:, 1]\npred_xgbc = xgbc.predict_proba(X_fold2)[:, 1]\npred_rf = rf.predict_proba(X_fold2)[:, 1]\n\navg_pred = (pred_logreg + pred_xgbc + pred_rf) / 3\n\n\nfold2_preds = np.column_stack((pred_logreg, \n                               pred_xgbc, \n                               pred_rf,\n                              avg_pred))\n\nauc_fold2 = []\nfor i in range(fold2_preds.shape[1]):\n    auc = metrics.roc_auc_score(y_fold2, fold2_preds[:,i])\n    auc_fold2.append(auc)\n    \nprint(f\"Fold-2:LR AUC = {auc_fold2[0]}\")\nprint(f\"Fold-2:RF AUC = {auc_fold2[1]}\")\nprint(f\"Fold-2:XGB AUC = {auc_fold2[2]}\")\nprint(f\"Fold-2:Average Pred AUC = {auc_fold2[3]}\")\n\nlogreg = linear_model.LogisticRegression()\nrf = ensemble.RandomForestClassifier()\nxgbc = xgb.XGBClassifier()\n\nlogreg.fit(X_fold2, y_fold2)\nrf.fit(X_fold2, y_fold2)\nxgbc.fit(X_fold2, y_fold2)\n\npred_logreg = logreg.predict_proba(X_fold1)[:, 1]\npred_xgbc = xgbc.predict_proba(X_fold1)[:, 1]\npred_rf = rf.predict_proba(X_fold1)[:, 1]\n\navg_pred = (pred_logreg + pred_xgbc + pred_rf) / 3\n\n\nfold1_preds = np.column_stack((pred_logreg, \n                               pred_xgbc, \n                               pred_rf,\n                              avg_pred))\n\nauc_fold1 = []\nfor i in range(fold1_preds.shape[1]):\n    auc = metrics.roc_auc_score(y_fold1, fold1_preds[:, i])\n    auc_fold1.append(auc)\n    \nprint(f\"Fold-1:LR AUC = {auc_fold1[0]}\")\nprint(f\"Fold-1:RF AUC = {auc_fold1[1]}\")\nprint(f\"Fold-1:XGB AUC = {auc_fold1[2]}\")\nprint(f\"Fold-1:Average Pred AUC = {auc_fold1[3]}\")\n\nopt = OptimizeAUC()\nopt.fit(fold1_preds[:, :-1], y_fold1)\nopt_preds_fold2 = opt.predict(fold2_preds[:,:-1])\nauc = metrics.roc_auc_score(y_fold2, opt_preds_fold2)\n\nprint(f\"Optimized AUC, Fold 2 = {auc}\")\nprint(f\"Coefficients = {opt.coef_}\")\n\nopt = OptimizeAUC()\nopt.fit(fold2_preds[:, :-1], y_fold2)\nopt_preds_fold1 = opt.predict(fold1_preds[:, :-1])\nauc = metrics.roc_auc_score(y_fold1, opt_preds_fold1)\nprint(f\"Optimized AUC, Fold 1 = {auc}\")\nprint(f\"Coefficients = {opt.coef_}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:50:07.152798Z","iopub.execute_input":"2021-05-29T10:50:07.153423Z","iopub.status.idle":"2021-05-29T10:50:14.102546Z","shell.execute_reply.started":"2021-05-29T10:50:07.153389Z","shell.execute_reply":"2021-05-29T10:50:14.101605Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[10:50:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFold-2:LR AUC = 0.9819228771076604\nFold-2:RF AUC = 0.9932872789259646\nFold-2:XGB AUC = 0.9923511987761919\nFold-2:Average Pred AUC = 0.9918726386996221\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[10:50:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nFold-1:LR AUC = 0.9878219180515069\nFold-1:RF AUC = 0.9954627192740352\nFold-1:XGB AUC = 0.9954403992704638\nFold-1:Average Pred AUC = 0.995201119232179\nOptimization terminated successfully.\n         Current function value: -0.995642\n         Iterations: 41\n         Function evaluations: 87\nOptimized AUC, Fold 2 = 0.9926998388319743\nCoefficients = [-0.02345547  0.04296401  0.27837574]\nOptimization terminated successfully.\n         Current function value: -0.992966\n         Iterations: 39\n         Function evaluations: 82\nOptimized AUC, Fold 1 = 0.995186719229875\nCoefficients = [-0.02957242  0.62947233  0.14069287]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Scikit-Learn Example\n\nThis is a more clean example of using voting between models. ","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\nX, y  = make_classification(n_samples=10_000, n_features=25)\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    voting='hard')\nclfs_and_names = zip([clf1, clf2, clf3, eclf], \n    ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble'])\n\nfor clf, label in clfs_and_names:\n    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=2)\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:14:20.232876Z","iopub.execute_input":"2021-05-29T11:14:20.233280Z","iopub.status.idle":"2021-05-29T11:14:25.510961Z","shell.execute_reply.started":"2021-05-29T11:14:20.233248Z","shell.execute_reply":"2021-05-29T11:14:25.509994Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Accuracy: 0.86 (+/- 0.01) [Logistic Regression]\nAccuracy: 0.92 (+/- 0.01) [Random Forest]\nAccuracy: 0.87 (+/- 0.01) [naive Bayes]\nAccuracy: 0.87 (+/- 0.01) [Ensemble]\n","output_type":"stream"}]}]}