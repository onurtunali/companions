{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dietary-premiere",
   "metadata": {},
   "source": [
    "# Approaching Categorical Variables II\n",
    "\n",
    "Now that we have covered all the preprocessing of categorical features, we can use these techniques in model training and evaluation. Contrary to the book, native scikit-learn classes and methods are used for pipelines and cross validation.\n",
    "\n",
    "- **Preprocessing:** This step is run through a pipeline and feature distinction is performed automatically using `make_column_selector` method. This is important because inproduction, data ingestion and processing step needs to automated.\n",
    "- **Model evaluation:** Scikit-learn provides a cross validation function yielding scores instead of preparing folds manually. In addition, different folding strategies can be passed to the function `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7278de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing numeric_transform, total=   0.7s\n",
      "[ColumnTransformer]  (2 of 2) Processing categoric_transform, total=   2.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72555338, 0.71580379, 0.73621327, 0.73979058, 0.74117373])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "M = 20_000 # Sample size\n",
    "\n",
    "df = pd.read_csv(\"data/catinthedat_train.csv\").drop(\"id\", axis=1)\n",
    "df_X = df.drop(\"target\", axis=1)\n",
    "df_y = df.target.values\n",
    "\n",
    "categorical_columns = df_X.select_dtypes(exclude=\"number\").columns\n",
    "df_X[categorical_columns] = df_X[categorical_columns].astype(\"category\")\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"numeric_transform\", simple_imputer, selector(dtype_exclude=\"category\")),\n",
    "        (\"categoric_transform\", one_hot_encoder, selector(dtype_include=\"category\")),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "X_transformed = ct.fit_transform(df_X)\n",
    "\n",
    "linear_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "cross_val_score(linear_regression, X_transformed[:M,:], df_y[:M], n_jobs=-1, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6a8f",
   "metadata": {},
   "source": [
    "Book implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    df = pd.read_csv('data/catinthedat_train_folds.csv')\n",
    "    \n",
    "    features = [ f for f in df.columns if f not in ['id','target','kfold']]\n",
    "    \n",
    "    for col in features:\n",
    "        df.loc[:,col] = df[col].astype(str).fillna('NONE')\n",
    "        \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    df_val = df[df.kfold == fold].reset_index(drop=True)\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    full_data = pd.concat([df_train[features], df_val[features]], axis=0)\n",
    "    ohe.fit(full_data[features])\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    x_valid = ohe.transform(df_val[features])\n",
    "    \n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    model.fit(X=x_train, y=df_train.target.values)\n",
    "    valid_predictions = model.predict_proba(x_valid)[:,1]\n",
    "    auc = metrics.roc_auc_score(df_val.target.values, valid_predictions)\n",
    "    print(f'Fold = {fold}, Accuracy = {auc}')\n",
    "    return x_train, x_valid, model\n",
    "    \n",
    "for fold_ in range(1):\n",
    "    x_train, x_valid, model = run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer]  (1 of 2) Processing numeric_transform, total=   0.4s\n",
      "[ColumnTransformer]  (2 of 2) Processing categoric_transform, total=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:59:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80795, 0.8095 , 0.80755, 0.80645, 0.8138 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "M = 100_000  # Sample size\n",
    "\n",
    "df = pd.read_csv(\"data/catinthedat_train.csv\").drop(\"id\", axis=1)\n",
    "df_X = df.drop(\"target\", axis=1)\n",
    "df_y = df.target.values.astype(int)\n",
    "\n",
    "categorical_columns = df_X.select_dtypes(exclude=\"number\").columns\n",
    "df_X[categorical_columns] = df_X[categorical_columns].astype(str).fillna(\"None\")\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "feature_encoder = OrdinalEncoder()\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"numeric_transform\", simple_imputer, selector(dtype_include=\"number\")),\n",
    "        (\"categoric_transform\", feature_encoder, selector(dtype_exclude=\"number\"))\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    # n_jobs=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "X_transformed = ct.fit_transform(df_X)\n",
    "\n",
    "xtreme_gradient_boosting = xgb.XGBClassifier(\n",
    "    n_jobs=-1,\n",
    "    max_depth=7,\n",
    "    n_estimators=200,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "\n",
    "cross_val_score(\n",
    "    xtreme_gradient_boosting,\n",
    "    X_transformed[:M, :],\n",
    "    df_y[:M],\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-difficulty",
   "metadata": {},
   "source": [
    "## US Adult Census Data\n",
    "\n",
    "- This is a somewhat more manageable dataset consisting of few numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adult = pd.read_csv('data/adult.csv')\n",
    "df_adult = df_adult.rename(columns={'income':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "df_adult = df_adult.sample(frac=1).reset_index(drop=True)\n",
    "df_adult['kfold'] = -1\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "y = df_adult.target.values\n",
    "\n",
    "for fold_, (train_, val_) in enumerate(kf.split(df_adult, y=y)):\n",
    "    df_adult.loc[val_, 'kfold'] = fold_\n",
    "\n",
    "df_adult.to_csv('data/adult_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# nnum_features = [f for f in df_adult.columns \n",
    "#                  if f not in ['age','fnlwgt','education.num',\n",
    "#                                                'capital.gain','capital.loss',\n",
    "#                                                'hours.per.week','kfold']]\n",
    "\n",
    "# for col in nnum_features:\n",
    "#     lbl_encoder = LabelEncoder()\n",
    "#     lbl_encoder.fit(df_adult[col])\n",
    "#     df_adult[col] = lbl_encoder.transform(df_adult[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "def run(fold):\n",
    "    df = pd.read_csv('data/adult_folds.csv')\n",
    "\n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "    \"fnlwgt\",\n",
    "    \"age\",\n",
    "    \"capital.gain\",\n",
    "    \"capital.loss\",\n",
    "    \"hours.per.week\",\n",
    "    ]\n",
    "    target_maps = {'<=50K':0,'>50K':1}\n",
    "    df.loc[:,'target'] = df.target.map(target_maps)\n",
    "    # drop numerical columns\n",
    "#     df = df.drop(num_cols, axis=1)\n",
    "    \n",
    "    features = [f for f in df.columns if f not in ['kfold','target']]\n",
    "    \n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            df.loc[:,col] = df[col].astype(str).fillna('NONE')\n",
    "            lbl_encoder = LabelEncoder()\n",
    "            lbl_encoder.fit(df[col])\n",
    "            df[col] = lbl_encoder.transform(df[col])\n",
    "    \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    x_train = df_train[features].values\n",
    "    y_train = df_train.target.values\n",
    "    \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    x_valid = df_valid[features].values\n",
    "    y_valid = df_valid.target.values\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=100\n",
    "        )\n",
    "#     model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(y_valid, valid_preds)\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "for fold in range(5):\n",
    "    run(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a feature engineering functionf\n",
    "\n",
    "def feature_eng(df, cat_cols):\n",
    "    import itertools\n",
    "    \n",
    "    feat_comb = list(itertools.combinations(cat_cols,2))\n",
    "    \n",
    "    for c1,c2 in feat_comb:\n",
    "        feat_name = f'{c1}_{c2}'\n",
    "        df.loc[:,feat_name] = df[c1].astype(str) + '_' + df[c2].astype(str)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "def run(fold):\n",
    "    df = pd.read_csv('data/adult_folds.csv')\n",
    "\n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "    \"fnlwgt\",\n",
    "    \"age\",\n",
    "    \"capital.gain\",\n",
    "    \"capital.loss\",\n",
    "    \"hours.per.week\",\n",
    "    ]\n",
    "    \n",
    "    cat_cols = [f for f in df.columns \n",
    "                if f not in num_cols and f not in ['kfold','target'] ]\n",
    "    \n",
    "    target_maps = {'<=50K':0,'>50K':1}\n",
    "    df.loc[:,'target'] = df.target.map(target_maps)\n",
    "    # drop numerical columns\n",
    "#     df = df.drop(num_cols, axis=1)\n",
    "    df = feature_eng(df, cat_cols)\n",
    "    \n",
    "    features = [f for f in df.columns if f not in ['kfold','target']]\n",
    "    \n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            df.loc[:,col] = df[col].astype(str).fillna('NONE')\n",
    "            lbl_encoder = LabelEncoder()\n",
    "            lbl_encoder.fit(df[col])\n",
    "            df[col] = lbl_encoder.transform(df[col])\n",
    "    \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    x_train = df_train[features].values\n",
    "    y_train = df_train.target.values\n",
    "    \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    x_valid = df_valid[features].values\n",
    "    y_valid = df_valid.target.values\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=100\n",
    "        )\n",
    "#     model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(y_valid, valid_preds)\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "for fold in range(5):\n",
    "    run(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c8fe3",
   "metadata": {},
   "source": [
    "# Entity Embedding\n",
    "\n",
    "When number of categorical features increases, transformed matrices might have enormous number of columns. That's why we need another way to denote categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bafa3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 15:52:24.204239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:24.253593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:24.254071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:24.255867: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-11 15:52:24.256345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:24.256839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:24.257239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:25.285070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:25.285388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:25.285633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-11 15:52:25.286343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1613 MB memory:  -> device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model\" expects 1 input(s), but it received 23 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:13' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:16' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:17' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:18' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:19' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:20' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:21' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:22' shape=(None,) dtype=int64>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=115'>116</a>\u001b[0m     \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mroc_auc_score(y_validation, valid_preds))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=116'>117</a>\u001b[0m     K\u001b[39m.\u001b[39mclear_session()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=118'>119</a>\u001b[0m run(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb Cell 14'\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=103'>104</a>\u001b[0m y_valid_cat \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_categorical(y_validation)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=104'>105</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=106'>107</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=107'>108</a>\u001b[0m     X_train,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=108'>109</a>\u001b[0m     y_train_cat,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=109'>110</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_validation, y_valid_cat),\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=110'>111</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=111'>112</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=112'>113</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=113'>114</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=114'>115</a>\u001b[0m valid_preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_valid)[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/onur/companions/aaamlp/6_approaching_categorical_variables_ii.ipynb#ch0000013?line=115'>116</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mroc_auc_score(y_validation, valid_preds))\n",
      "File \u001b[0;32m~/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/onur/companions/aaamlp/venv/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model\" expects 1 input(s), but it received 23 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:10' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:11' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:12' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:13' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:14' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:15' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:16' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:17' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:18' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:19' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:20' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:21' shape=(None,) dtype=int64>, <tf.Tensor 'IteratorGetNext:22' shape=(None,) dtype=int64>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "\n",
    "def create_model(data, catcols):\n",
    "    \"\"\"\n",
    "    This function returns a compiled tf.keras model\n",
    "    for entity embeddings\n",
    "    :param data: this is a pandas dataframe\n",
    "    :param catcols: list of categorical column names\n",
    "    :return: compiled tf.keras model\n",
    "    \"\"\"\n",
    "    # init list of inputs for embeddings\n",
    "    inputs = []\n",
    "    # init list of outputs for embeddings\n",
    "    outputs = []\n",
    "    # loop over all categorical columns\n",
    "    for c in catcols:\n",
    "        # find the number of unique values in the column\n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        # simple dimension of embedding calculator\n",
    "        # min size is half of the number of unique values\n",
    "        # max size is 50. max size depends on the number of unique\n",
    "        # categories too. 50 is quite sufficient most of the times\n",
    "        # but if you have millions of unique values, you might need\n",
    "        # a larger dimension\n",
    "        embed_dim = int(min(np.ceil((num_unique_values) / 2), 50))\n",
    "        # simple keras input layer with size 1\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        # add embedding layer to raw input\n",
    "        # embedding size is always 1 more than unique values in input\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        # 1-d spatial dropout is the standard for emebedding layers\n",
    "        # you can use it in NLP tasks too\n",
    "        out = layers.SpatialDropout1D(0.3)(out)\n",
    "        # reshape the input to the dimension of embedding\n",
    "        # this becomes our output layer for current feature\n",
    "        out = layers.Reshape(target_shape=(embed_dim,))(out)\n",
    "\n",
    "        # add input to input list\n",
    "        inputs.append(inp)\n",
    "        # add output to output list\n",
    "\n",
    "        outputs.append(out)\n",
    "        # concatenate all output layers\n",
    "        x = layers.Concatenate()(outputs)\n",
    "\n",
    "        # x add a batchnorm layer.\n",
    "        # from here, everything is up to you\n",
    "        # you can try different architectures\n",
    "        # this is the architecture I like to use\n",
    "        # if you have numerical features, you should add\n",
    "        # them here or in concatenate layer\n",
    "\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Dense(300, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Dense(300, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = Model(inputs=inputs, outputs=y)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "        return model\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    df = pd.read_csv(\"data/catinthedat_train_folds.csv\").sample(frac=0.2).reset_index(drop=True)\n",
    "    features = [f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"None\")\n",
    "\n",
    "    for col in features:\n",
    "        feature_encoder = preprocessing.LabelEncoder()\n",
    "        df.loc[:, col] = feature_encoder.fit_transform(df[col].values)\n",
    "\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_validation = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    model = create_model(df, features)\n",
    "\n",
    "    X_train = tuple([df_train[features].values[:, k] for k in range(len(features))])\n",
    "    X_validation = tuple([df_validation[features].values[:, k] for k in range(len(features))])\n",
    "\n",
    "    y_train = df_train.target.values\n",
    "    y_validation = df_validation.target.values\n",
    "\n",
    "    y_train_cat = utils.to_categorical(y_train)\n",
    "    y_valid_cat = utils.to_categorical(y_validation)\n",
    "    # fit the model\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        validation_data=(X_validation, y_valid_cat),\n",
    "        verbose=1,\n",
    "        batch_size=1024,\n",
    "        epochs=1,\n",
    "    )\n",
    "    valid_preds = model.predict(x_valid)[:,1]\n",
    "    print(metrics.roc_auc_score(y_validation, valid_preds))\n",
    "    K.clear_session()\n",
    "\n",
    "run(1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c402e9d03c480f6a7135df87527cfd3970e19e56cccf54d43d96144c8d60506d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
