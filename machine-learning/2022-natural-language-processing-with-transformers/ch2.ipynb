{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Text Classification\n",
    "\n",
    "Since 2022 (publication date), a lot has changed for Huggingface. There are thousands of datasets at the moment, therefore listing all datasets is kind of meaningless.\n",
    "\n",
    "Yet if you want to perform this operation, current approach is using `huggingface_hub` method that return a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data types have specific methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")\n",
    "methods = [i for i in dir(emotions[\"train\"].features[\"label\"]) if \"_\" not in i]\n",
    "print(f\"{methods=}\")\n",
    "\n",
    "emotions[\"train\"].features[\"label\"].str2int(\"joy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2str(label):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(label)\n",
    "\n",
    "\n",
    "df[\"label_name\"] = df.label.apply(label2str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=300)\n",
    "\n",
    "df.label_name.value_counts(ascending=True).plot.barh(ax=ax)\n",
    "\n",
    "plt.title(\"Label Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=300)\n",
    "\n",
    "df[\"token_size\"] = df.text.str.split().str.len()\n",
    "df.boxplot(column=\"token_size\", by=\"label_name\", grid=False, ax=ax, color=\"black\")\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"Label Name\")\n",
    "plt.ylabel(\"Token Size\")\n",
    "plt.title(\"Token Size Distribution by Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "print(f\"{text=}\")\n",
    "\n",
    "text_dict = tokenizer(text)\n",
    "text_ids = text_dict[\"input_ids\"]\n",
    "print(f\"{text_ids=}\")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(text_ids)\n",
    "print(f\"{tokens=}\")\n",
    "\n",
    "token2string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(f\"{token2string=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tokenizer.vocab_size=} and {tokenizer.model_max_length=} and {tokenizer.model_input_names=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "print(tokenize_batch(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we don't tokenize the texts without batch_size since the whole dataset will be padded to the longest sequence in the entire dataset. However, in this example we will use hidden states as features and to generate these we inference in batches which requires the same sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize_batch, batched=True, batch_size=None, load_from_cache_file=False)\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModel.from_pretrained(model_checkpoint).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions for `last_hidden_state` is such that `(B, Si, D)` B is for batch, Si for sequence index and D is for embedding dimension. Zeroth sequence element is `[CLS]` token.\n",
    "\n",
    "For multi dimensional arrays, leaving dimensions imply take all of them. For an example with 4 dimensions `[:, 0] == [:, 0, :, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    last_hidden_state = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return {\"hidden_state\": last_hidden_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "\n",
    "x_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "\n",
    "x_test = np.array(emotions_hidden[\"test\"][\"hidden_state\"])\n",
    "y_test = np.array(emotions_hidden[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "x_scaled = MinMaxScaler().fit_transform(x_train)\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(x_scaled)\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"x\", \"y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(7, 5), dpi=500)\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb[df_emb.label == i]\n",
    "    axes[i].hexbin(df_emb_sub[\"x\"], df_emb_sub[\"y\"], cmap=cmap, gridsize=20, linewidths=(20,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(x_train, y_train)\n",
    "lr_clf.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(x_train, y_train)\n",
    "dummy_clf.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "\n",
    "def draw_confusion_matrix(y_true, y_preds, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", colorbar=False)\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "y_pred = lr_clf.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(y_valid, y_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(emotions[\"train\"].features[\"label\"].names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"models/{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=emotions_encoded[\"train\"],\n",
    "    eval_dataset=emotions_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "draw_confusion_matrix(y_valid, y_preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), reduction=\"none\")\n",
    "\n",
    "    return {\"loss\": loss.cpu().numpy(), \"predicted_label\": pred_label.cpu().numpy()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batch_size=64, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label_name\"] = df_test.label.apply(label2str)\n",
    "df_test[\"predicted_label_name\"] = df_test.predicted_label.apply(label2str)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"models/distilbert-base-uncased-finetuned-emotion/checkpoint-500\"\n",
    "classifier = pipeline(task=\"text-classification\", model=model_id)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classifier(\"This is abysmal! I have never seen such a pointless movie in my life.\", return_all_scores=True)\n",
    "\n",
    "scores = pd.DataFrame(scores[0])\n",
    "scores\n",
    "plt.bar(labels, 100 * scores[\"score\"], color=\"C0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.predict(emotions_encoded[\"train\"].select(range(10)))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
