{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Hello Transformers\n",
    "\n",
    "Before transformers, recurrent neural network (RNN) and long short-term memory (LSTM) models are used for sequence to sequence tasks. These approaches consist of encoder an decoder blocks such that the whole context is sequeezed into the final hidden state. In order to expand access to whole hidden states, **attention** mechanism is introduced in (Bahdanau et al., 2014).\n",
    "\n",
    "<img src=\"assets/ch1/1.png\" width=750>\n",
    "\n",
    "\n",
    "**Transformer** architecture removed the sequential nature and introduced **self-attention** mechanism in (Vaswani et al., 2017). By the way, transformer name is literal. Model transforms given sequence into another sequence.\n",
    "\n",
    "Inspired by the vision field's pretraining and transfer learning approach, NLP researchers also designed a task independent approach for language modeling. Unsupervised training with generative approach in (Radford et al., 2017) achieved good results for producing a base model. ULMFIT paper (Howard and Ruder, 2014) goes like this: language modelling with pretraining => domain adaptation => task specific fine tuning. Using this approach, instead of model training with large amount of data for each task, pretrained model is used as a base model and only small amount of labeled data is used for task dependent fine tuning.\n",
    "\n",
    "Finally, two work force of modern ML models are introduced: encoder only Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2018) and decoder only Generative Pretrained Transformer (GPT) (Radford et al., 2018)\n",
    "\n",
    "# References\n",
    "\n",
    "- D. Bahdanau et al., \"Neural machine translation by jointly learning to align and translate\", 2014.\n",
    "- A. Vaswani et al., \"Attention is all you need\", 2017.\n",
    "- A. Radford et al., “Learning to Generate Reviews and Discovering Sentiment”, 2017.\n",
    "- J. Howard and S. Ruder, \"Universal language model fine-tuning for text classification.\", 2018.\n",
    "- J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, 2018.\n",
    "- A. Radford et al., “Improving Language Understanding by Generative Pre-Training”, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
    "from your online store in Germany. Unfortunately, when I opened the package,\n",
    "I discovered to my horror that I had been sent an action figure of Megatron\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    revision=\"714eb0f\",\n",
    ")\n",
    "pd.DataFrame(classifier(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\n",
    "    task=\"ner\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    revision=\"4c53496\",\n",
    ")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"distilbert/distilbert-base-cased-distilled-squad\",\n",
    "    revision=\"564e9b5\",\n",
    ")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    revision=\"a4f8f3e\",\n",
    ")\n",
    "outputs = summarizer(text, max_length=45, min_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\n",
    "    task=\"translation_en_to_de\",\n",
    "    model=\"google-t5/t5-base\",\n",
    "    revision=\"a9723ea\",\n",
    ")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0][\"translation_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
