{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Transformer Anatomy\n",
    "\n",
    "There are 3 types of transfomers:\n",
    "- **Encoder only**: Bidirectional attention\n",
    "- **Decoder only**: Autoregressive or causal attention\n",
    "- **Encoder-Decoder**\n",
    "\n",
    "## Calculating Attention and Contextualized embedding \n",
    "\n",
    "### 1. Embedding Matrix $E$ $(5 \\times 4)$\n",
    "\n",
    "The input sequence of 5 tokens, each with an embedding dimension of 4, is represented as:\n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}\n",
    "e_{11} & e_{12} & e_{13} & e_{14} \\\\\n",
    "e_{21} & e_{22} & e_{23} & e_{24} \\\\\n",
    "e_{31} & e_{32} & e_{33} & e_{34} \\\\\n",
    "e_{41} & e_{42} & e_{43} & e_{44} \\\\\n",
    "e_{51} & e_{52} & e_{53} & e_{54}\n",
    "\\end{bmatrix}\n",
    "E \\in \\mathbb{R}^{5 \\times 4}\n",
    "$$\n",
    "\n",
    "### 2. Weight Matrices $W_q$, $W_k$, and $W_v (4 \\times 2)$ \n",
    "\n",
    "The weight matrices for the query, key, and value transformations are:\n",
    "\n",
    "$$\n",
    "W_q = \\begin{bmatrix}\n",
    "w_{11}^q & w_{12}^q \\\\\n",
    "w_{21}^q & w_{22}^q \\\\\n",
    "w_{31}^q & w_{32}^q \\\\\n",
    "w_{41}^q & w_{42}^q\n",
    "\\end{bmatrix}, \\quad\n",
    "W_k = \\begin{bmatrix}\n",
    "w_{11}^k & w_{12}^k \\\\\n",
    "w_{21}^k & w_{22}^k \\\\\n",
    "w_{31}^k & w_{32}^k \\\\\n",
    "w_{41}^k & w_{42}^k\n",
    "\\end{bmatrix}, \\quad\n",
    "W_v = \\begin{bmatrix}\n",
    "w_{11}^v & w_{12}^v \\\\\n",
    "w_{21}^v & w_{22}^v \\\\\n",
    "w_{31}^v & w_{32}^v \\\\\n",
    "w_{41}^v & w_{42}^v\n",
    "\\end{bmatrix}\n",
    "W_q, W_k, W_v \\in \\mathbb{R}^{4 \\times 2}\n",
    "$$\n",
    "\n",
    "### 3. Query $Q$, Key $K$, and Value $V$ Matrices $(5 \\times 2)$\n",
    "\n",
    "The query, key, and value matrices are computed as:\n",
    "\n",
    "$$\n",
    "Q = E W_q, \\quad K = E W_k, \\quad V = E W_v\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "q_{11} & q_{12} \\\\\n",
    "q_{21} & q_{22} \\\\\n",
    "q_{31} & q_{32} \\\\\n",
    "q_{41} & q_{42} \\\\\n",
    "q_{51} & q_{52}\n",
    "\\end{bmatrix}, \\quad\n",
    "K = \\begin{bmatrix}\n",
    "k_{11} & k_{12} \\\\\n",
    "k_{21} & k_{22} \\\\\n",
    "k_{31} & k_{32} \\\\\n",
    "k_{41} & k_{42} \\\\\n",
    "k_{51} & k_{52}\n",
    "\\end{bmatrix}, \\quad\n",
    "V = \\begin{bmatrix}\n",
    "v_{11} & v_{12} \\\\\n",
    "v_{21} & v_{22} \\\\\n",
    "v_{31} & v_{32} \\\\\n",
    "v_{41} & v_{42} \\\\\n",
    "v_{51} & v_{52}\n",
    "\\end{bmatrix}\n",
    "Q, K, V \\in \\mathbb{R}^{5 \\times 2}\n",
    "$$\n",
    "\n",
    "### 4. Attention Scores with Softmax\n",
    "\n",
    "The attention scores are computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention Scores} = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)\n",
    "$$\n",
    "\n",
    "where $d = 2$ (the dimension of the key vectors). The matrix multiplication $QK^T$ results in a $5 \\times 5$ matrix:\n",
    "\n",
    "$$\n",
    "QK^T = \\begin{bmatrix}\n",
    "q_{11}k_{11} + q_{12}k_{12} & q_{11}k_{21} + q_{12}k_{22} & \\cdots & q_{11}k_{51} + q_{12}k_{52} \\\\\n",
    "q_{21}k_{11} + q_{22}k_{12} & q_{21}k_{21} + q_{22}k_{22} & \\cdots & q_{21}k_{51} + q_{22}k_{52} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "q_{51}k_{11} + q_{52}k_{12} & q_{51}k_{21} + q_{52}k_{22} & \\cdots & q_{51}k_{51} + q_{52}k_{52}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After scaling by $\\frac{1}{\\sqrt{d}}$ and applying the softmax function row-wise, the attention scores matrix $A$ is:\n",
    "\n",
    "$$\n",
    "A = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{2}}\\right)\n",
    "A \\in \\mathbb{R}^{5 \\times 5}\n",
    "$$\n",
    "\n",
    "### 5. Attention Output\n",
    "\n",
    "The final output of the self-attention mechanism is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Output} = A V\n",
    "$$\n",
    "\n",
    "Explicitly:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\begin{bmatrix}\n",
    "a_{11}v_{11} + a_{12}v_{21} + \\cdots + a_{15}v_{51} & a_{11}v_{12} + a_{12}v_{22} + \\cdots + a_{15}v_{52} \\\\\n",
    "a_{21}v_{11} + a_{22}v_{21} + \\cdots + a_{25}v_{51} & a_{21}v_{12} + a_{22}v_{22} + \\cdots + a_{25}v_{52} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "a_{51}v_{11} + a_{52}v_{21} + \\cdots + a_{55}v_{51} & a_{51}v_{12} + a_{52}v_{22} + \\cdots + a_{55}v_{52}\n",
    "\\end{bmatrix}\n",
    "\\text{Output} \\in \\mathbb{R}^{5 \\times 2}\n",
    "$$\n",
    "\n",
    "Since Bert has 12 attention heads in following visualization, Q,K and V matrices has a column size of 64.\n",
    "\n",
    "<img src=\"assets/ch3/1.png\" width=750>\n",
    "\n",
    "Basically, contextualized embedding of $x$ denoted with $x{'}$ is a linear combination of all value matrix rows using attention scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from bertviz.neuron_view import show\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "text = \"Time flies like an arrow\"\n",
    "\n",
    "show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query, key and value terminology comes from Information Retrieval field. When everything is text, it's harder to understand why we need them, however an example in Chollet's \"Deep Learning with Python\" book demonstrates clearly when the key and values are different in terms of modality.\n",
    "\n",
    "<img src=\"assets/ch3/2.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def scaled_dot_product(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    if mask:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)  # W_q\n",
    "        self.k = nn.Linear(embed_dim, head_dim)  # W_k\n",
    "        self.v = nn.Linear(embed_dim, head_dim)  # W_v\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        Q = self.q(hidden_state)\n",
    "        K = self.q(hidden_state)\n",
    "        V = self.q(hidden_state)\n",
    "        attn_outputs = scaled_dot_product(Q, K, V)\n",
    "        return attn_outputs\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attetion_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.outpu_layer = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.outpu_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        x = x + self.attention(hidden_state)\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embedding(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cross attention, source and target might have different dimensions therefore attention scores are rectangular:\n",
    "\n",
    "$$ \n",
    "SE \\in \\mathbb{R}^{4 \\times 4}, \\quad \\text{source embeddings} \\\\\n",
    "TE \\in \\mathbb{R}^{8 \\times 4}, \\quad \\text{target embeddings}\n",
    "$$\n",
    "\n",
    "Query matrix passed to the attention comes from decoder (target) and key, value comes from encoder (source).\n",
    "```python\n",
    "\n",
    "query = TE * W_tq\n",
    "key = SE * W_sk\n",
    "value = SE * W_sv\n",
    "# Rectangular attention scores\n",
    "scaled_dot_product(query, key, value, mask=None):\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
