{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6112a3fd",
   "metadata": {},
   "source": [
    "# 1 The Machine Learning Landscape\n",
    "\n",
    "Machine Learning (ML) is making a computer accomplish something without explicitly programming it. Instead of giving input and logic to the program directly to get the data, input and data are given to the program to get the logic. \n",
    "\n",
    "> **Remark:** Before starting every machine learning project, the most important questions to ask \n",
    "> - Can we do this without ML?\n",
    "> - What is the business objective and how to measure performance?\n",
    "> - How does performance boost look like?\n",
    "\n",
    "\n",
    "## 1.1 Types of ML\n",
    "\n",
    "We can roughly use 3 criteria to divide ML methods int categories\n",
    "- Supervised and Unsupervised Learning\n",
    "- Batch and Online Learning\n",
    "- Instance or Model Based Learning\n",
    "\n",
    "\n",
    "### Supervised and Unsupervised Learning\n",
    "\n",
    "- **Supervised Learning**: Classification, regression.\n",
    "- **Semi-supervised Learning**: \n",
    "- **Unsupervised Learning**: Clustering, density estimation, dimensionality reduction.\n",
    "- **Reinforcement Learning**:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "During dimensionality reduction, we can use highly correlated features and combined them into one which is called feature extraction. For example, `age` and `milage` of a car is strongly correlated. Therefore, we can call this new feature `wear_and_tear`.\n",
    "\n",
    "### Batch and Online Learning\n",
    "\n",
    "**Batch learning (offline learning)** processes the whole dataset and **online learning** processes dataset as they come and adjusts the model. When online learning is used to train with huge datasets, this can be accomplished with out-of-core learning that is done offline usually. Incremental learning would be a better name for it. \n",
    "\n",
    "\n",
    "### Instance or Model Based Learning\n",
    "\n",
    "Another important distinction about a learning algorithm whether it's instance or model based. Instance based approach compares the new instance with dataset through some similarity metric and decides the results. Model base approach learn pattern from data and constructs a model. The decision is made through model.\n",
    "\n",
    "\n",
    "## 1.2 Main Challenges of ML\n",
    "\n",
    "Humans are remarkable at pattern recognition as opposed to machines which require lots of data. Main problem with learning systems obviously related to data. Following are the main issues:\n",
    "\n",
    "- Insufficient quantity of data\n",
    "- Nonrepresentative data (sampling bias)\n",
    "- Poor quality data (the bulk of data engineering and processing step to clean data)\n",
    "- Irrelevant features\n",
    "- Overfitting and underfitting model\n",
    "\n",
    "Especially deep learning models require gigantic amount of data. Since they automate representation learning this is excepted and little off topic to be honest just a reminder. \n",
    "\n",
    "> **Remark:** *The Unreasonable effectiveness of data* [[1]]() is quite a high level paper about how more data helps in terms of performance metrics rather than clever new algorithms. It's revisited in 2017 regarding deep learning methods by the paper *Revisiting unreasonable effectiveness of data in deep learning era* [[2]]() released by Google. The paper uses an internal (not public) enormous dataset called JFT. Their main findings are as follows: \n",
    "> - **a)** Large data improves [representation learning](https://en.wikipedia.org/wiki/Feature_learning) \n",
    "> - **b)** performance gain is logarithmic \n",
    "> - **c)** Model capacity needs to be adjusted according to data in terms of size \n",
    "> - **d)** even with long tail data, performance is not severely affected. \n",
    "\n",
    "## 1.3 Testing and Validating\n",
    "\n",
    "This is the most important step to measure performance of ML approach. We test the model with never before seen data and validate how correct the results are. This is called holdout testing. However, in addition to model parameters, we also need to adjust hyper-parameters that are used to choose model itself. For this step, cross validation is used. In short, we divide training set into k parts and then we train the model with k-1 set and run the test on the left out set. Repeating this process k times, we can toggle hyper-parameters.\n",
    "\n",
    "<img src=\"fig/chapter1/validation.svg\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0dd94",
   "metadata": {},
   "source": [
    "## 1.4 References\n",
    "\n",
    "[[1]](#alon2009unreasonable) Halevy, Alon, Peter Norvig, and Fernando Pereira. *\"The unreasonable effectiveness of data.\"* IEEE Intelligent Systems 24.2 .2009: 8-12.\n",
    "\n",
    "[[2]](#alon2017revisiting) Sun, Chen, et al. *\"Revisiting unreasonable effectiveness of data in deep learning era.\"* Proceedings of the IEEE international conference on computer vision. 2017.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
