{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fc8287",
   "metadata": {},
   "source": [
    "# 10 Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ml-test\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnbDF = spark.read.format(\"parquet\").load(\"data/sf-airbnb/sf-airbnb-clean.parquet\")\n",
    "airbnbDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78617163",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnbDF.columns\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \"number_of_reviews\", \"price\").show(5)\n",
    "airbnbDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a222b5",
   "metadata": {},
   "source": [
    "Since executors perform their own partitions, setting up `seed` is not enought for generating the same split. For this reason `trainDF` and `testDF` must be save after the split and used from reloading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([0.8, 0.2], seed=1337)\n",
    "\n",
    "f\"Train data size {trainDF.count()} and test size {testDF.count()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a703fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vec_train_df = vec_assembler.transform(trainDF)\n",
    "vec_test_df = vec_assembler.transform(testDF)\n",
    "\n",
    "vec_train_df.select(\"bedrooms\", \"features\", \"price\").show()\n",
    "vec_train_df.corr(\"bedrooms\", \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(vec_train_df)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vec_assembler, lr])\n",
    "pipeline_model = pipeline.fit(trainDF)\n",
    "pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pipeline_model.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30dc32",
   "metadata": {},
   "source": [
    "So far we have only used numerical features. In order to use categorical variables without introducing and inherent order, we can use `StringIndexer` and `OneHotEncoder`. An important aspect is handling the values of categorical variables not present in train data. We can explicitly decide how to handle these cases with `handleInvalid` parameter of `StringIndexer`:\n",
    "- `'error'`: default value, throw an error\n",
    "- `'skip'`: skip data points not present in labels\n",
    "- `'keep'`: assign the last value of the index. If the cardinality of categorical variable is n, then indices are $0, \\dots, n-1$. Unknown values are indexed as $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ace7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"spec\", outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "train = spark.createDataFrame(\n",
    "    data=[\n",
    "        [0, \"cat\"],\n",
    "        [0, \"dog\"],\n",
    "        [0, \"cat\"],\n",
    "        [0, \"cat\"],\n",
    "    ],\n",
    "    schema=[\"id\", \"spec\"],\n",
    ")\n",
    "\n",
    "test = spark.createDataFrame(\n",
    "    data=[\n",
    "        [0, \"cat\"],\n",
    "        [0, \"dog\"],\n",
    "        [0, \"cat\"],\n",
    "        [0, \"bird\"],\n",
    "    ],\n",
    "    schema=[\"id\", \"spec\"],\n",
    ")\n",
    "string_indexer = string_indexer.fit(train)\n",
    "test = string_indexer.transform(test)\n",
    "# test.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754704fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for field, dtype in trainDF.dtypes if dtype == \"string\"]\n",
    "\n",
    "indexOutputCols = [field + \"Index\" for field in categoricalCols]\n",
    "oheOutputCols = [field + \"OHE\" for field in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"keep\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "numericalCols = [field for field, dtype in trainDF.dtypes if (dtype == \"double\") & (field != \"price\")]\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericalCols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2214bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vec_assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a66c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDf = pipelineModel.transform(testDF)\n",
    "predDf.select(\"features\", \"price\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n",
    "r2 = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"r2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit, mean\n",
    "\n",
    "price_avg = trainDF.select(mean(col(\"price\")).alias(\"avg_price\")).toPandas()[\"avg_price\"].item()\n",
    "predDf = predDF.withColumn(\"avg_prediction\", lit(price_avg))\n",
    "predDf.select(\"prediction\", \"avg_prediction\").show()\n",
    "\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"avg_prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDf)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"avg_prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDf)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2845e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainDF.select(\"price\").toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d51c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9774dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"price\").hist(bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"log_values\"] = np.log(df.price.values)\n",
    "df.log_values.hist(bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.write().overwrite().save(\"temp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "temp_model = PipelineModel.load(\"temp\")\n",
    "temp_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
