{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956cc11c",
   "metadata": {},
   "source": [
    "# 4 Spark SQL and Dataframes: Introduction to Built-in Data Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879efe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "data_path_csv = \"data/flights/departuredelays.csv\"\n",
    "\n",
    "df: DataFrame = spark.read.option(\"SampleRatio\", 0.1).csv(data_path_csv, header=True, inferSchema=True)\n",
    "df.count(), df.columns, df.show(5), df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc36da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from temp_table limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"select * from temp_table where distance > 1000 order by distance desc\").show()\n",
    "spark.sql(\"select * from temp_table where delay > 120 and origin = 'SFO' and destination = 'ORD'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df2 = df.withColumn(\"date_str\", F.lpad(F.col(\"date\").cast(\"string\"), 8, \"0\"))\n",
    "\n",
    "# Now parse based on the apparent MMDDHHmm format\n",
    "df2 = df2.withColumn(\"timestamp\", F.to_timestamp(F.col(\"date_str\"), \"MMddHHmm\"))\n",
    "\n",
    "# Show results\n",
    "df2.select(\"date\", \"timestamp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select delay, origin, destination, \n",
    "    Case\n",
    "        WHEN delay > 360 THEN 'Very Long Delays'\n",
    "        WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "        WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "        WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "        WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "    END as Flight_delays\n",
    "from temp_table\n",
    "order by origin, delay DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc755585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\n",
    "    \"Flight_delays\",\n",
    "    F.when(F.col(\"delay\") > 360, \"Very Long Delays\")\n",
    "    .when((F.col(\"delay\") < 360) & (F.col(\"delay\") > 120), \"Long Delays\")\n",
    "    .otherwise(\"Early\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953651a",
   "metadata": {},
   "source": [
    "There are 2 types of tables:\n",
    "- **Managed**: Spark manages both the metadata and data itself.\n",
    "- **Unmanaged**: Spark only manages metadata\n",
    "\n",
    "Deleting a managed table erases both the metada and data. On the contrary, deleting an unmanaged table only erases metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"CREATE TABLE managed_temp_table (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "df.write.saveAsTable(\"managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from managed_table limit 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceef775",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.select(\"origin\").distinct().toPandas()\n",
    "destinations = a.origin.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sampleBy(\"origin\", fractions={i: 0.1 for i in destinations}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255c5ea",
   "metadata": {},
   "source": [
    "Temporary and global temporary views have the following distinction:\n",
    "- Temporary view can only be used by a single `SparkSession`\n",
    "- Global temporary view can be used by multiple `SparkSession`\n",
    "\n",
    "A single application might have more than one `SparkSession`. This is useful when you need to access data with different Hive Metastore configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa219e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = df.select(\"date\", \"delay\", \"origin\", \"destination\").filter(F.col(\"origin\") == \"SFO\")\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb20e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from global_temp.us_origin_airport_SFO_global_tmp_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160acf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema\n",
    "\n",
    "schema_ddl = \"`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\"\n",
    "\n",
    "flights = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema_ddl).load(\"data/flights/departuredelays.csv\")\n",
    "flights.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
